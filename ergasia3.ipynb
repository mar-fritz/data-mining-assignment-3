{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Εργασία 3 (Τεχνικές Εξόρυξης Δεδομένων)\n",
    "## Data Mining: Assignment 3\n",
    "***\n",
    "### Μαρία Φριτζελά 1115201400218\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from unicodedata import normalize\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import  svm, metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Collection and cleaning of data (Pre-processing text)\n",
    "Date information is not needed so it is not added to our dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(\"data/train.csv\", usecols=['Insult', 'Comment'])\n",
    "testdf = pd.read_csv(\"data/impermium_verification_labels.csv\", index_col='id', usecols=['id', 'Insult', 'Comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at traindf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>1</td>\n",
       "      <td>\"you are both morons and that is never happening\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Many toolbars include spell check, like Yahoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>\"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>\"How about Felix? He is sure turning into one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>\"You're all upset, defending this hipster band...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3947 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult                                            Comment\n",
       "0          1                               \"You fuck your dad.\"\n",
       "1          0  \"i really don't understand your point.\\xa0 It ...\n",
       "2          0  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...\n",
       "3          0  \"listen if you dont wanna get married to a man...\n",
       "4          0  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...\n",
       "...      ...                                                ...\n",
       "3942       1  \"you are both morons and that is never happening\"\n",
       "3943       0  \"Many toolbars include spell check, like Yahoo...\n",
       "3944       0  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...\n",
       "3945       0  \"How about Felix? He is sure turning into one ...\n",
       "3946       0  \"You're all upset, defending this hipster band...\n",
       "\n",
       "[3947 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the train and test data to X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = traindf.Comment, traindf.Insult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = testdf.Comment, testdf.Insult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up train comments' text:\n",
    "- convert all letters to lowercase\n",
    "- remove multiple instances of `\\`<br>\n",
    "For example \"\\\\\\n\" becomes \"\\n\"\n",
    "- remove \"\\n\" and \"\\xa0\" (non-breaking space latin)\n",
    "- remove usernames\n",
    "- remove URLs\n",
    "- remove special unicode characters (like \\xe1, \\xe2...)<br>\n",
    "- remove puctuation\n",
    "- remove all words containing digits, and any digits\n",
    "- remove multiple spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comments(comments):\n",
    "    return comments.apply(lambda comment: comment.lower())\\\n",
    "                .apply(lambda comment: re.sub(\"\\\\\\\\{2,}\", \" \\\\\\\\\" ,comment))\\\n",
    "                .apply(lambda comment: re.sub(\"\\\\\\\\+n|\\\\\\\\+xa0\", \" \", comment))\\\n",
    "                .apply(lambda comment: re.sub('@\\S+',' ',comment))\\\n",
    "                .apply(lambda comment: re.sub('(http(s)?:\\/\\/|www\\.)(\\S|[a-z]|[A-Z]| [0-9])+', \" \", comment))\\\n",
    "                .apply(lambda comment: re.sub('\\\\\\\\+\\S+',\" \", comment))\\\n",
    "                .apply(lambda comment: re.sub('[^A-Za-z0-9 ]+', ' ',comment))\\\n",
    "                .apply(lambda comment: re.sub(r'\\w*\\d\\w*', '', comment))\\\n",
    "                .apply(lambda comment: re.sub(r\"\\s+\",\" \", comment, flags = re.I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = clean_comments(traindf.Comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = clean_comments(testdf.Comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example this comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Nope. Not working for me either.32-23-34www.facebook.com/annagillmodel\\\\\\\\n\\\\\\\\n \\\\\\\\n\\\\\\\\nYou have my email! :) \"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.Comment[124]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has been transformed into this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' nope not working for me either you have my email '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[124]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cleaned up data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      you fuck your dad \n",
       "1        i really don t understand your point it seems...\n",
       "2        a majority of canadians can and has been wron...\n",
       "3        listen if you dont wanna get married to a man...\n",
       "4        c b xu bi t c ho kh c ng d ng cu chi nh c ho ...\n",
       "                              ...                        \n",
       "3942     you are both morons and that is never happening \n",
       "3943     many toolbars include spell check like yahoo ...\n",
       "3944     sioux falls s d i told my boy he should call ...\n",
       "3945     how about felix he is sure turning into one h...\n",
       "3946     you re all upset defending this hipster band ...\n",
       "Name: Comment, Length: 3947, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Naive Bayes\n",
    "\n",
    "Transform the comments into word count vectors using CountVectorizer from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bag-of-words vector\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the vector of the first comment in the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dad</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuck</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aamir</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firms</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firmly</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firing</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fired</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuckerberg</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5873 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            counts\n",
       "you              1\n",
       "dad              1\n",
       "fuck             1\n",
       "your             1\n",
       "aamir            0\n",
       "...            ...\n",
       "firms            0\n",
       "firmly           0\n",
       "firing           0\n",
       "fired            0\n",
       "zuckerberg       0\n",
       "\n",
       "[5873 rows x 1 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_bow[0:1].T.todense(), index=bow_vectorizer.get_feature_names(), columns=[\"counts\"])\\\n",
    ".sort_values(by=[\"counts\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Train the model on the BoW training set\n",
    "nb.fit(X_train_bow.toarray(), y_train)\n",
    "# predict the BoW test set\n",
    "y_pred_nb_bow = nb.predict(X_test_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold Cross Validation Precision NB for BoW: 0.5924473946218446\n",
      "10-fold Cross Validation Recall NB for BoW: 0.6073470556111351\n",
      "10-fold Cross Validation F-Measure NB for BoW: 0.593717216685966\n",
      "10-fold Cross Validation Accuracy NB for BoW: 0.6534100109233438\n"
     ]
    }
   ],
   "source": [
    "print(\"10-fold Cross Validation Precision NB for BoW:\",\n",
    "      np.mean(cross_val_score(nb, X_train_bow.toarray(), y_train, cv=10, scoring='precision_macro')))\n",
    "print(\"10-fold Cross Validation Recall NB for BoW:\",\n",
    "      np.mean(cross_val_score(nb, X_train_bow.toarray(), y_train, cv=10, scoring='recall_macro')))\n",
    "print(\"10-fold Cross Validation F-Measure NB for BoW:\",\n",
    "     np.mean(cross_val_score(nb, X_train_bow.toarray(), y_train, cv=10, scoring='f1_macro')))\n",
    "print(\"10-fold Cross Validation Accuracy NB for BoW:\",\n",
    "      np.mean(cross_val_score(nb, X_train_bow.toarray(), y_train, cv=10, scoring='accuracy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision NB for BoW: [0.54675468 0.5015083 ]\n",
      "Recall NB for BoW: [0.42918826 0.6174559 ]\n",
      "F-Measure NB for BoW: [0.48089018 0.55347482]\n",
      "\n",
      "Accuracy NB for BoW: 0.519910514541387\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision NB for BoW:\",metrics.precision_score(y_test, y_pred_nb_bow, average=None))\n",
    "print(\"Recall NB for BoW:\",metrics.recall_score(y_test, y_pred_nb_bow, average=None))\n",
    "print(\"F-Measure NB for BoW:\", metrics.f1_score(y_test, y_pred_nb_bow, average=None))\n",
    "print()\n",
    "print(\"Accuracy NB for BoW:\",metrics.accuracy_score(y_test,y_pred_nb_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are not very good... Let's improve them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the scores of Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use lemmatization of words to improve the scores from the previous question, using the WordNetLemmatizer from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "X_train_lem = X_train.apply(lambda item: ' '.join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(item)]))\n",
    "X_test_lem = X_test.apply(lambda item: ' '.join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(item)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bag-of-words vector\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train_lem)\n",
    "X_test_bow = bow_vectorizer.transform(X_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the lemmatized BoW training set\n",
    "nb.fit(X_train_bow.toarray(), y_train)\n",
    "# predict the BoW test set\n",
    "y_pred_nb_bow = nb.predict(X_test_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure NB for Lemmatized BoW data: [0.51469923 0.52501107]\n",
      "\n",
      "Accuracy NB for Lemmatized BoW data: 0.519910514541387\n"
     ]
    }
   ],
   "source": [
    "print(\"F-Measure NB for Lemmatized BoW data:\", metrics.f1_score(y_test, y_pred_nb_bow, average=None))\n",
    "print()\n",
    "print(\"Accuracy NB for Lemmatized BoW data:\",metrics.accuracy_score(y_test,y_pred_nb_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Stop word filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a bag-of-words vector, removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bag-of-words vector\n",
    "bow_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the stopword free BoW training set\n",
    "nb.fit(X_train_bow.toarray(), y_train)\n",
    "# predict the BoW test set\n",
    "y_pred_nb_bow = nb.predict(X_test_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure NB for stopword free BoW data: [0.52037618 0.5212338 ]\n",
      "\n",
      "Accuracy NB for stopword free BoW data: 0.5208053691275167\n"
     ]
    }
   ],
   "source": [
    "print(\"F-Measure NB for stopword free BoW data:\", metrics.f1_score(y_test, y_pred_nb_bow, average=None))\n",
    "print()\n",
    "print(\"Accuracy NB for stopword free BoW data:\",metrics.accuracy_score(y_test,y_pred_nb_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Use of bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a bag-of-words vector, including bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bag-of-words vector for only bigrams\n",
    "bow_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the bigrams BoW training set\n",
    "nb.fit(X_train_bow.toarray(), y_train)\n",
    "# predict the BoW test set\n",
    "y_pred_nb_bow = nb.predict(X_test_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure NB for bigrams BoW data: [0.58853077 0.52763095]\n",
      "\n",
      "Accuracy NB for bigrams BoW data: 0.5601789709172259\n"
     ]
    }
   ],
   "source": [
    "print(\"F-Measure NB for bigrams BoW data:\", metrics.f1_score(y_test, y_pred_nb_bow, average=None))\n",
    "print()\n",
    "print(\"Accuracy NB for bigrams BoW data:\",metrics.accuracy_score(y_test,y_pred_nb_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bag-of-words vector\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a=1 is called Laplace smoothing\n",
    "\n",
    "_(https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "mnb = MultinomialNB(alpha=1.0)\n",
    "\n",
    "# Train the model on the BoW training set\n",
    "mnb.fit(X_train_bow.toarray(), y_train)\n",
    "# predict the BoW test set\n",
    "y_pred_mnb_bow = mnb.predict(X_test_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure NB for BoW: [0.71803018 0.63627049]\n",
      "\n",
      "Accuracy NB for BoW: 0.6823266219239373\n"
     ]
    }
   ],
   "source": [
    "print(\"F-Measure NB for BoW:\", metrics.f1_score(y_test, y_pred_mnb_bow, average=None))\n",
    "print()\n",
    "print(\"Accuracy NB for BoW:\",metrics.accuracy_score(y_test,y_pred_mnb_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying different combinations of configurations, this seems like the optimal setting for the highest accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bag-of-words vector\n",
    "bow_vectorizer = CountVectorizer(min_df=2, ngram_range=(1,2), stop_words='english')\n",
    "\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the BoW training set\n",
    "mnb.fit(X_train_bow.toarray(), y_train)\n",
    "# predict the BoW test set\n",
    "y_pred_mnb_bow = mnb.predict(X_test_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold Cross Validation Precision NB for BoW: 0.7389453859092777\n",
      "10-fold Cross Validation Recall NB for BoW: 0.792935771671752\n",
      "10-fold Cross Validation F-Measure NB for BoW: 0.7479243192090579\n",
      "10-fold Cross Validation Accuracy NB for BoW: 0.7762892758465592\n"
     ]
    }
   ],
   "source": [
    "print(\"10-fold Cross Validation Precision NB for BoW:\",\n",
    "      np.mean(cross_val_score(mnb, X_train_bow.toarray(), y_train, cv=10, scoring='precision_macro')))\n",
    "print(\"10-fold Cross Validation Recall NB for BoW:\",\n",
    "      np.mean(cross_val_score(mnb, X_train_bow.toarray(), y_train, cv=10, scoring='recall_macro')))\n",
    "print(\"10-fold Cross Validation F-Measure NB for BoW:\",\n",
    "     np.mean(cross_val_score(mnb, X_train_bow.toarray(), y_train, cv=10, scoring='f1_macro')))\n",
    "print(\"10-fold Cross Validation Accuracy NB for BoW:\",\n",
    "      np.mean(cross_val_score(mnb, X_train_bow.toarray(), y_train, cv=10, scoring='accuracy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision NB for BoW: [0.66136835 0.74492386]\n",
      "Recall NB for BoW: [0.82642487 0.5450325 ]\n",
      "F-Measure NB for BoW: [0.73474088 0.62949062]\n",
      "\n",
      "Accuracy NB for BoW: 0.69082774049217\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision NB for BoW:\",metrics.precision_score(y_test, y_pred_mnb_bow, average=None))\n",
    "print(\"Recall NB for BoW:\",metrics.recall_score(y_test, y_pred_mnb_bow, average=None))\n",
    "print(\"F-Measure NB for BoW:\", metrics.f1_score(y_test, y_pred_mnb_bow, average=None))\n",
    "print()\n",
    "print(\"Accuracy NB for BoW:\",metrics.accuracy_score(y_test,y_pred_mnb_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! **~20%** improvement in scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, a table of combinations of settings tested along with their accuracy scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| NB accuracy Score  | # of features | Lemmatization | Stopwords removal | n-gram_range | Laplace Smoothing |\n",
    "|:------------------:|:-------------:|:-------------:|:-----------------:|:------------:|:-----------------:|\n",
    "|        0.519       |  14220 (all)  |       N       |         N         |     (1,1)    |         N         |\n",
    "|        0.510       |      4000     |       N       |         N         |     (1,1)    |         N         |\n",
    "|       0.5199       |    min_df=2   |       N       |         N         |     (1,1)    |         N         |\n",
    "|       0.5176       |    min_df=2   |       Y       |         N         |     (1,1)    |         N         |\n",
    "|       0.5208       |    min_df=2   |       N       |         Y         |     (1,1)    |         N         |\n",
    "|        0.519       |    min_df=2   |       Y       |         Y         |     (1,1)    |         N         |\n",
    "|       0.5297       |    min_df=2   |       Y       |         Y         |     (1,2)    |         N         |\n",
    "|       0.5364       |    min_df=2   |       N       |         Y         |     (1,2)    |         N         |\n",
    "|        0.489       |    min_df=2   |       N       |         Y         |     (2,2)    |         N         |\n",
    "|        0.508       |      all      |       N       |         Y         |     (2,2)    |         N         |\n",
    "|     **0.6908**     |    min_df=2   |       N       |         Y         |     (1,2)    |         Y         |\n",
    "|        0.688       |    min_df=2   |       Y       |         Y         |     (1,2)    |         Y         |\n",
    "|        0.63        |      all      |       N       |         Y         |     (1,2)    |         Y         |\n",
    "|        0.652       |    min_df=2   |       N       |         N         |     (1,2)    |         Y         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**<br>\n",
    "- After trying different number of features (all, max=400, min_df=2) it appears that the best accuracy score is achieved by ignoring terms that have a document frequency strictly lower than 2. This leaves us with 5873 features.\n",
    "\n",
    "- The use of lemmatization brings the accuracy score down. This can be explained because when analysing insults, the result might differ depending on the form of the word and therefore the input should not be stemmed or lemmatized. _(Example: \"this situation sucks, man\" (no insult) vs \"you suck\" (insult))_\n",
    "\n",
    "- Stopwords removal causes an increase in the score, as well as using an n-gram range of (1,2) (unigrams and bigrams)\n",
    "\n",
    "- Finally, applying Laplace smoothing results in great increase of accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of a custom feature vector: TF/IDF Vector & Part-of-Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech frequency features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use nltk's pos_tag method for each word of every comment in our data. Set the tagset attribute to 'universal' in the pos_tag method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tagged = X_train.apply(lambda item: nltk.pos_tag(nltk.word_tokenize(item), tagset='universal'))\n",
    "X_test_tagged = X_test.apply(lambda item: nltk.pos_tag(nltk.word_tokenize(item), tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the result looks like for comment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NOUN'),\n",
       " ('really', 'ADV'),\n",
       " ('don', 'ADJ'),\n",
       " ('t', 'NOUN'),\n",
       " ('understand', 'VERB'),\n",
       " ('your', 'PRON'),\n",
       " ('point', 'NOUN'),\n",
       " ('it', 'PRON'),\n",
       " ('seems', 'VERB'),\n",
       " ('that', 'ADP'),\n",
       " ('you', 'PRON'),\n",
       " ('are', 'VERB'),\n",
       " ('mixing', 'VERB'),\n",
       " ('apples', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('oranges', 'NOUN')]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tagged[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency distribution (`nltk.FreqDist`) can be defined as a function mapping from each sample to the number of times that sample occurred as an outcome.<br>\n",
    "It will be used to record the frequency of each word type in each comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NOUN': 5, 'VERB': 4, 'PRON': 3, 'ADV': 1, 'ADJ': 1, 'ADP': 1, 'CONJ': 1})"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(tag for word, tag in X_train_tagged[1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function fractPOS:** Creates a list of dictionaries iterating through every comment passed in X_tagged. Each dictionary holds the fraction (=frequency_of_tag/number_of_words_in_comment) of each tag for that comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractPOS(X_tagged):\n",
    "    fractions = []\n",
    "    for tagged_comment in X_tagged:\n",
    "        n_of_words = len(tagged_comment)\n",
    "        freq = nltk.FreqDist(tag for word, tag in tagged_comment)\n",
    "        # freq[tag_type] if a tag type doesn't exist, zero is returned\n",
    "        try:\n",
    "            d = {\n",
    "                'ADV': freq['ADV']/n_of_words,\n",
    "                'VERB': freq['VERB']/n_of_words,\n",
    "                'ADJ': freq['ADJ']/n_of_words,\n",
    "                'NOUN': freq['NOUN']/n_of_words\n",
    "            }\n",
    "        except ZeroDivisionError: #n_of_words ==0\n",
    "            d = {'ADV': 0, 'VERB': 0, 'ADJ': 0,'NOUN': 0}\n",
    "        fractions.append(d)\n",
    "    return fractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe is created using the list of dictionaries.<br>\n",
    "_We chose not to fill the dataframe row by row, because iteratively appending rows to a DataFrame can be computationally intensive_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['ADV', 'VERB', 'ADJ', 'NOUN']\n",
    "X_train_freqdf = pd.DataFrame(fractPOS(X_train_tagged), columns=tags)\n",
    "X_test_freqdf = pd.DataFrame(fractPOS(X_test_tagged), columns=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our custom feature vector (dataframe) for the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADV</th>\n",
       "      <th>VERB</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>NOUN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.202899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.118644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.244444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3947 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ADV      VERB       ADJ      NOUN\n",
       "0     0.000000  0.250000  0.000000  0.250000\n",
       "1     0.062500  0.250000  0.062500  0.312500\n",
       "2     0.086957  0.202899  0.057971  0.202899\n",
       "3     0.033898  0.305085  0.084746  0.118644\n",
       "4     0.000000  0.047619  0.063492  0.809524\n",
       "...        ...       ...       ...       ...\n",
       "3942  0.111111  0.333333  0.000000  0.111111\n",
       "3943  0.076923  0.346154  0.076923  0.153846\n",
       "3944  0.000000  0.307692  0.076923  0.269231\n",
       "3945  0.111111  0.222222  0.027778  0.277778\n",
       "3946  0.022222  0.222222  0.044444  0.244444\n",
       "\n",
       "[3947 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_freqdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF/IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a TF/IDF vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer= TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dad</th>\n",
       "      <td>0.794537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuck</th>\n",
       "      <td>0.481069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>0.308020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.205932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaah</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flying</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flynn</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>focking</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>focus</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuckerberg</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14220 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               tfidf\n",
       "dad         0.794537\n",
       "fuck        0.481069\n",
       "your        0.308020\n",
       "you         0.205932\n",
       "aaaah       0.000000\n",
       "...              ...\n",
       "flying      0.000000\n",
       "flynn       0.000000\n",
       "focking     0.000000\n",
       "focus       0.000000\n",
       "zuckerberg  0.000000\n",
       "\n",
       "[14220 rows x 1 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_tfidf[0:1].T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\\\n",
    ".sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the custom part-of-speech features with the TF/IDF vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF vector is a matrix where the rows are comments and the columns are features.<br>\n",
    "To combine all features, the custom features will be added as columns to the end of the TF/IDF matrix.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF/IDF matrix is a sparse matrix (from Scipy).<br>\n",
    "To save memory, do not convert to dense, rather use `scipy.sparse.hstack` to stack the matrices horizontally (column wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = hstack([X_train_tfidf, X_train_freqdf])\n",
    "X_test_combined = hstack([X_test_tfidf, X_test_freqdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined matrix consists of 3947 rows (same as the number of comments in the train dataset) and 14220 features from TFIDF + 4 custom part-of-speech features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3947, 14224)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "_(https://stackoverflow.com/questions/48573174/how-to-combine-tfidf-features-with-other-features)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the assigment details, scoring should be calculated calculated using: classification accuracy and F1 score.\n",
    "\n",
    "Find optimal parameters for the SVM model as shown in this example:<br>\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for f1_macro\n",
      "Best parameters set found on development set:\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "# Tuning hyper-parameters for accuracy\n",
      "Best parameters set found on development set:\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['f1_macro', 'accuracy']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        svm.SVC(), tuned_parameters, scoring=score\n",
    "    )\n",
    "    clf.fit(X_train_combined, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(clf.best_params_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the model\n",
    "svm_clf = svm.SVC(C=1, kernel='linear')\n",
    "\n",
    "# train the model on the custom training set\n",
    "svm_clf.fit(X_train_combined, y_train)\n",
    "# predict the custom test set\n",
    "y_pred_svm = svm_clf.predict(X_test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold Cross Validation F-Measure SVM for custom feature vector: 0.7716475529688832\n",
      "10-fold Cross Validation Accuracy SVM for custom feature vector: 0.8386172331812635\n"
     ]
    }
   ],
   "source": [
    "print(\"10-fold Cross Validation F-Measure SVM for custom feature vector:\",\n",
    "     np.mean(cross_val_score(svm_clf, X_train_combined, y_train, cv=10, scoring='f1_macro')))\n",
    "print(\"10-fold Cross Validation Accuracy SVM for custom feature vector:\",\n",
    "      np.mean(cross_val_score(svm_clf, X_train_combined, y_train, cv=10, scoring='accuracy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure SVM for custom feature vector: [0.75598086 0.62179122]\n",
      "Accuracy SVM for custom feature vector: 0.7033557046979866\n"
     ]
    }
   ],
   "source": [
    "print(\"F-Measure SVM for custom feature vector:\", metrics.f1_score(y_test, y_pred_svm, average=None))\n",
    "print(\"Accuracy SVM for custom feature vector:\",metrics.accuracy_score(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Train the model on the custom training set\n",
    "rf.fit(X_train_combined, y_train)\n",
    "# predict the custom test set\n",
    "y_pred_rf = rf.predict(X_test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold Cross Validation F-Measure RF for custom feature vector: 0.6588474815828835\n",
      "10-fold Cross Validation Accuracy RF for custom feature vector: 0.8016205101844116\n"
     ]
    }
   ],
   "source": [
    "print(\"10-fold Cross Validation F-Measure RF for custom feature vector:\",\n",
    "     np.mean(cross_val_score(rf, X_train_combined, y_train, cv=10, scoring='f1_macro')))\n",
    "print(\"10-fold Cross Validation Accuracy RF for custom feature vector:\",\n",
    "      np.mean(cross_val_score(rf, X_train_combined, y_train, cv=10, scoring='accuracy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure RF for custom feature vector: [0.73501474 0.42907551]\n",
      "Accuracy RF for custom feature vector: 0.6380313199105145\n"
     ]
    }
   ],
   "source": [
    "print(\"F-Measure RF for custom feature vector:\", metrics.f1_score(y_test, y_pred_rf, average=None))\n",
    "print(\"Accuracy RF for custom feature vector:\",metrics.accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the custom training set\n",
    "mnb.fit(X_train_combined.toarray(), y_train)\n",
    "# predict the custom test set\n",
    "y_pred_mnb_bow = mnb.predict(X_test_combined.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure NB for custom feature vector: [0.69154972 0.07850134]\n",
      "\n",
      "Accuracy NB for custom feature vector: 0.537807606263982\n"
     ]
    }
   ],
   "source": [
    "print(\"F-Measure NB for custom feature vector:\", metrics.f1_score(y_test, y_pred_mnb_bow, average=None))\n",
    "print()\n",
    "print(\"Accuracy NB for custom feature vector:\",metrics.accuracy_score(y_test,y_pred_mnb_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beat the benchmark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to improve the scores on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for creating the combined features vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the classifiers for different data preprocessing options we create a function which returns the combined test and train vector. (TF/IDF + Part of Speech features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **lemm**: (True/False) whether or not we want to lemmatize the text data before creating the TFIDF vector\n",
    "- **tfidfSW**: ('english'/None) what type of stopwords to use for TFIDF vector\n",
    "- **ngram_r**: the ngram_range of the TFIDF vector\n",
    "- **posSW**: (True/False) whether to keep stopwords before counting the PoS \n",
    "- **svd**: (True/False) whether or not to Perform dimensionality reduction (LSA via TruncatedSVD) on the sparse PoS data to make it dense and combine the features into a single dense matrix. \n",
    "_(because the features from the TF/IDF matrix are be sparse, meaning it will contain a lot of 0s. Whilst the PoS feature vector is dense and continuous.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which creates the combined feature vector with different parameters\n",
    "# lemm = (True/False) whether or not we want to lemmatize the data before creating the TFIDF vector\n",
    "# tfidfSW = ('english'/None) what type of stopwords to use for TFIDF vector\n",
    "# ngram_r = the ngram_range of the TFIDF vector\n",
    "# posSW = (True/False) whether to keep stopwords when counting the PoS \n",
    "# svd = whether or not to Perform dimensionality reduction (LSA via TruncatedSVD) on the sparse PoS data \n",
    "# to make it dense and combine the features into a single dense matrix\n",
    "\n",
    "def create_combined(lemm, tfidfSW, ngram_r, posSW, svd,  X_train, X_test):\n",
    "    #print(\"Preparing features...\")\n",
    "    #print(\"PoS stopwords: \"+str(posSW)+\" TFIDF Lemmatization: \"+str(lemm))\n",
    "    #print(\"TFIDF: ngram_range=\"+str(ngram_r)+\" stopwords=\"+str(tfidfSW))\n",
    "    #print(\"SVD=\"+str(svd))\n",
    "    if posSW: #if we want stopwords in our PoS features\n",
    "        X_train_tagged = X_train.apply(lambda item: nltk.pos_tag(nltk.word_tokenize(item), tagset='universal'))\n",
    "        X_test_tagged = X_test.apply(lambda item: nltk.pos_tag(nltk.word_tokenize(item), tagset='universal'))\n",
    "    else:\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        X_train_noSW = X_train.apply(lambda item: ' '.join(list(filter(lambda word: word not in stop_words, item.split()))))\n",
    "        X_test_noSW = X_test.apply(lambda item: ' '.join(list(filter(lambda word: word not in stop_words, item.split()))))\n",
    "        X_train_tagged = X_train_noSW.apply(lambda item: nltk.pos_tag(nltk.word_tokenize(item), tagset='universal'))\n",
    "        X_test_tagged = X_test_noSW.apply(lambda item: nltk.pos_tag(nltk.word_tokenize(item), tagset='universal'))\n",
    "    tags = ['ADV', 'VERB', 'ADJ', 'NOUN']\n",
    "    X_train_freqdf = pd.DataFrame(fractPOS(X_train_tagged), columns=tags)\n",
    "    X_test_freqdf = pd.DataFrame(fractPOS(X_test_tagged), columns=tags)\n",
    "    \n",
    "    tfidf_vectorizer= TfidfVectorizer(ngram_range=ngram_r, stop_words=tfidfSW)\n",
    "    if lemm: #if we want lemmatization in our TFIDF features\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        X_train_lem = X_train.apply(lambda item: ' '.join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(item)]))\n",
    "        X_test_lem = X_test.apply(lambda item: ' '.join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(item)]))\n",
    "        \n",
    "        X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_lem)\n",
    "        X_test_tfidf = tfidf_vectorizer.transform(X_test_lem)\n",
    "    else:\n",
    "        X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "        \n",
    "    if svd:\n",
    "        svd = TruncatedSVD(n_components=4)\n",
    "\n",
    "        X_train_tfidf_svd = svd.fit_transform(X_train_tfidf)\n",
    "        X_test_tfidf_svd = svd.fit_transform(X_test_tfidf)\n",
    "        X_train_combined_ns = np.hstack([X_train_tfidf_svd, X_train_freqdf])\n",
    "        X_test_combined_ns = np.hstack([X_test_tfidf_svd, X_test_freqdf])\n",
    "        return X_train_combined_ns, X_test_combined_ns\n",
    "    \n",
    "    else:\n",
    "        X_train_combined = hstack([X_train_tfidf, X_train_freqdf])\n",
    "        X_test_combined = hstack([X_test_tfidf, X_test_freqdf])\n",
    "        return X_train_combined, X_test_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_rbf = []\n",
    "for lemm in [True, False]:\n",
    "    for tfidfSW in [None, 'english']:\n",
    "        for posSW in [True, False]:\n",
    "            for svd in [True, False]:\n",
    "                #print()\n",
    "                X_train_combined, X_test_combined = create_combined(lemm, tfidfSW, (1,2), posSW, svd,  X_train, X_test)\n",
    "                #instantiate the model\n",
    "                svm_clf = svm.SVC(C=1000, gamma=0.001, kernel='rbf')\n",
    "\n",
    "                # train the model on the custom training set\n",
    "                svm_clf.fit(X_train_combined, y_train)\n",
    "                # predict the custom test set\n",
    "                y_pred_svm = svm_clf.predict(X_test_combined)\n",
    "                \n",
    "                # scores\n",
    "                f1score = metrics.f1_score(y_test, y_pred_svm, average=None)\n",
    "                acc = metrics.accuracy_score(y_test, y_pred_svm)\n",
    "                #print(\"F-Measure SVM for custom feature vector:\", f1score)\n",
    "                #print(\"Accuracy SVM for custom feature vector:\", acc)\n",
    "                d = {\"PoS_SW\": posSW, \"TFIDF Lemmatization\":lemm,\n",
    "                    \"TFIDF: ngram_range\": (1,2), \"TFIDF_SW\": tfidfSW, \"SVD\": svd,\n",
    "                    \"F1 score\": f1score, \"Accuracy\":acc}\n",
    "                svm_rbf.append(d)\n",
    "svm_rbf_scores = pd.DataFrame(svm_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PoS_SW</th>\n",
       "      <th>TFIDF Lemmatization</th>\n",
       "      <th>TFIDF: ngram_range</th>\n",
       "      <th>TFIDF_SW</th>\n",
       "      <th>SVD</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6777117849327915, 0.18882769472856017]</td>\n",
       "      <td>0.538702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7477911646586345, 0.6828282828282829]</td>\n",
       "      <td>0.719016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6795752654590881, 0.19085173501577288]</td>\n",
       "      <td>0.540940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7485988791032825, 0.6815415821501014]</td>\n",
       "      <td>0.719016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6764982742390964, 0.19641465315666407]</td>\n",
       "      <td>0.538702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7548872180451128, 0.6397790055248619]</td>\n",
       "      <td>0.708277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6767106089139988, 0.19781931464174457]</td>\n",
       "      <td>0.539150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7548022598870057, 0.6413223140495867]</td>\n",
       "      <td>0.708725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6781142678738683, 0.186266771902131]</td>\n",
       "      <td>0.538702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.744484556758925, 0.6777946383409205]</td>\n",
       "      <td>0.714989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6783151326053043, 0.18498023715415018]</td>\n",
       "      <td>0.538702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7442046362909672, 0.6747967479674797]</td>\n",
       "      <td>0.713647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6969972702456778, 0.1483375959079284]</td>\n",
       "      <td>0.553020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7537764350453173, 0.6421514818880351]</td>\n",
       "      <td>0.708277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6969972702456778, 0.1483375959079284]</td>\n",
       "      <td>0.553020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7550866616428032, 0.6420704845814977]</td>\n",
       "      <td>0.709172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PoS_SW  TFIDF Lemmatization TFIDF: ngram_range TFIDF_SW    SVD  \\\n",
       "0     True                 True             (1, 2)     None   True   \n",
       "1     True                 True             (1, 2)     None  False   \n",
       "2    False                 True             (1, 2)     None   True   \n",
       "3    False                 True             (1, 2)     None  False   \n",
       "4     True                 True             (1, 2)  english   True   \n",
       "5     True                 True             (1, 2)  english  False   \n",
       "6    False                 True             (1, 2)  english   True   \n",
       "7    False                 True             (1, 2)  english  False   \n",
       "8     True                False             (1, 2)     None   True   \n",
       "9     True                False             (1, 2)     None  False   \n",
       "10   False                False             (1, 2)     None   True   \n",
       "11   False                False             (1, 2)     None  False   \n",
       "12    True                False             (1, 2)  english   True   \n",
       "13    True                False             (1, 2)  english  False   \n",
       "14   False                False             (1, 2)  english   True   \n",
       "15   False                False             (1, 2)  english  False   \n",
       "\n",
       "                                     F1 score  Accuracy  \n",
       "0   [0.6777117849327915, 0.18882769472856017]  0.538702  \n",
       "1    [0.7477911646586345, 0.6828282828282829]  0.719016  \n",
       "2   [0.6795752654590881, 0.19085173501577288]  0.540940  \n",
       "3    [0.7485988791032825, 0.6815415821501014]  0.719016  \n",
       "4   [0.6764982742390964, 0.19641465315666407]  0.538702  \n",
       "5    [0.7548872180451128, 0.6397790055248619]  0.708277  \n",
       "6   [0.6767106089139988, 0.19781931464174457]  0.539150  \n",
       "7    [0.7548022598870057, 0.6413223140495867]  0.708725  \n",
       "8     [0.6781142678738683, 0.186266771902131]  0.538702  \n",
       "9     [0.744484556758925, 0.6777946383409205]  0.714989  \n",
       "10  [0.6783151326053043, 0.18498023715415018]  0.538702  \n",
       "11   [0.7442046362909672, 0.6747967479674797]  0.713647  \n",
       "12   [0.6969972702456778, 0.1483375959079284]  0.553020  \n",
       "13   [0.7537764350453173, 0.6421514818880351]  0.708277  \n",
       "14   [0.6969972702456778, 0.1483375959079284]  0.553020  \n",
       "15   [0.7550866616428032, 0.6420704845814977]  0.709172  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_rbf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "- Stop words seem critical to understanding the actual meaning being considered, considering  SVM looks at the interactions between the features to a certain degree, when using a non-linear kernel (Gaussian, rbf, poly etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highest Accuracy score for SVM rbf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoS_SW                                                     True\n",
       "TFIDF Lemmatization                                        True\n",
       "TFIDF: ngram_range                                       (1, 2)\n",
       "TFIDF_SW                                                   None\n",
       "SVD                                                       False\n",
       "F1 score               [0.7477911646586345, 0.6828282828282829]\n",
       "Accuracy                                               0.719016\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_rbf_scores.loc[svm_rbf_scores['Accuracy'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_lin = []\n",
    "for lemm in [True, False]:\n",
    "    for tfidfSW in [None, 'english']:\n",
    "        for posSW in [True, False]:\n",
    "            for svd in [True, False]:\n",
    "                #print()\n",
    "                X_train_combined, X_test_combined = create_combined(lemm, tfidfSW, (1,2), posSW, svd,  X_train, X_test)\n",
    "                #instantiate the model\n",
    "                svm_clf = svm.SVC(C=1, kernel='linear')\n",
    "\n",
    "                # train the model on the custom training set\n",
    "                svm_clf.fit(X_train_combined, y_train)\n",
    "                # predict the custom test set\n",
    "                y_pred_svm = svm_clf.predict(X_test_combined)\n",
    "                \n",
    "                # scores\n",
    "                f1score = metrics.f1_score(y_test, y_pred_svm, average=None)\n",
    "                acc = metrics.accuracy_score(y_test, y_pred_svm)\n",
    "                #print(\"F-Measure SVM for custom feature vector:\", f1score)\n",
    "                #print(\"Accuracy SVM for custom feature vector:\", acc)\n",
    "                d = {\"PoS_SW\": posSW, \"TFIDF Lemmatization\":lemm,\n",
    "                    \"TFIDF: ngram_range\": (1,2), \"TFIDF_SW\": tfidfSW, \"SVD\": svd,\n",
    "                    \"F1 score\": f1score, \"Accuracy\":acc}\n",
    "                svm_lin.append(d)\n",
    "svm_lin_scores = pd.DataFrame(svm_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PoS_SW</th>\n",
       "      <th>TFIDF Lemmatization</th>\n",
       "      <th>TFIDF: ngram_range</th>\n",
       "      <th>TFIDF_SW</th>\n",
       "      <th>SVD</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6777117849327915, 0.18882769472856017]</td>\n",
       "      <td>0.538702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7477911646586345, 0.6828282828282829]</td>\n",
       "      <td>0.719016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6795752654590881, 0.19085173501577288]</td>\n",
       "      <td>0.540940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7485988791032825, 0.6815415821501014]</td>\n",
       "      <td>0.719016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6764982742390964, 0.19641465315666407]</td>\n",
       "      <td>0.538702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7548872180451128, 0.6397790055248619]</td>\n",
       "      <td>0.708277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6767106089139988, 0.19781931464174457]</td>\n",
       "      <td>0.539150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7548022598870057, 0.6413223140495867]</td>\n",
       "      <td>0.708725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6781142678738683, 0.186266771902131]</td>\n",
       "      <td>0.538702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.744484556758925, 0.6777946383409205]</td>\n",
       "      <td>0.714989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6783151326053043, 0.18498023715415018]</td>\n",
       "      <td>0.538702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7442046362909672, 0.6747967479674797]</td>\n",
       "      <td>0.713647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6969972702456778, 0.1483375959079284]</td>\n",
       "      <td>0.553020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7537764350453173, 0.6421514818880351]</td>\n",
       "      <td>0.708277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6969972702456778, 0.1483375959079284]</td>\n",
       "      <td>0.553020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7550866616428032, 0.6420704845814977]</td>\n",
       "      <td>0.709172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PoS_SW  TFIDF Lemmatization TFIDF: ngram_range TFIDF_SW    SVD  \\\n",
       "0     True                 True             (1, 2)     None   True   \n",
       "1     True                 True             (1, 2)     None  False   \n",
       "2    False                 True             (1, 2)     None   True   \n",
       "3    False                 True             (1, 2)     None  False   \n",
       "4     True                 True             (1, 2)  english   True   \n",
       "5     True                 True             (1, 2)  english  False   \n",
       "6    False                 True             (1, 2)  english   True   \n",
       "7    False                 True             (1, 2)  english  False   \n",
       "8     True                False             (1, 2)     None   True   \n",
       "9     True                False             (1, 2)     None  False   \n",
       "10   False                False             (1, 2)     None   True   \n",
       "11   False                False             (1, 2)     None  False   \n",
       "12    True                False             (1, 2)  english   True   \n",
       "13    True                False             (1, 2)  english  False   \n",
       "14   False                False             (1, 2)  english   True   \n",
       "15   False                False             (1, 2)  english  False   \n",
       "\n",
       "                                     F1 score  Accuracy  \n",
       "0   [0.6777117849327915, 0.18882769472856017]  0.538702  \n",
       "1    [0.7477911646586345, 0.6828282828282829]  0.719016  \n",
       "2   [0.6795752654590881, 0.19085173501577288]  0.540940  \n",
       "3    [0.7485988791032825, 0.6815415821501014]  0.719016  \n",
       "4   [0.6764982742390964, 0.19641465315666407]  0.538702  \n",
       "5    [0.7548872180451128, 0.6397790055248619]  0.708277  \n",
       "6   [0.6767106089139988, 0.19781931464174457]  0.539150  \n",
       "7    [0.7548022598870057, 0.6413223140495867]  0.708725  \n",
       "8     [0.6781142678738683, 0.186266771902131]  0.538702  \n",
       "9     [0.744484556758925, 0.6777946383409205]  0.714989  \n",
       "10  [0.6783151326053043, 0.18498023715415018]  0.538702  \n",
       "11   [0.7442046362909672, 0.6747967479674797]  0.713647  \n",
       "12   [0.6969972702456778, 0.1483375959079284]  0.553020  \n",
       "13   [0.7537764350453173, 0.6421514818880351]  0.708277  \n",
       "14   [0.6969972702456778, 0.1483375959079284]  0.553020  \n",
       "15   [0.7550866616428032, 0.6420704845814977]  0.709172  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_lin_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highest Accuracy score for SVM linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoS_SW                                                     True\n",
       "TFIDF Lemmatization                                        True\n",
       "TFIDF: ngram_range                                       (1, 2)\n",
       "TFIDF_SW                                                   None\n",
       "SVD                                                       False\n",
       "F1 score               [0.7477911646586345, 0.6828282828282829]\n",
       "Accuracy                                               0.719016\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_lin_scores.loc[svm_lin_scores['Accuracy'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a **1%** improvement from the previous linear svm accuracy score:\n",
    "\n",
    "_F-Measure SVM for custom feature vector: 0.75598086 0.62179122\n",
    "Accuracy SVM for custom feature vector: 0.7033557046979866_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features from the TF/IDF matrix are be sparse, meaning it will contain a lot of 0s. Whilst the PoS feature vector is dense and continuous. This will probably cause the prediction to be dominated by the dense variables.<br>\n",
    "<br>\n",
    "To combat this: perform dimensionality reduction (such as LSA via TruncatedSVD) on the sparse data to make it dense and combine the features into a single dense matrix.\n",
    "<br>\n",
    "SVM, RF and NB models are not additive models, they can combat the imbalance in frequency of sparse and dense features. This is why using the dimentionality reduction did not help with increasing their accuracy.\n",
    "<br>\n",
    "_(Further reading: https://datascience.stackexchange.com/questions/987/text-categorization-combining-different-kind-of-features )_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = []\n",
    "for lemm in [True, False]:\n",
    "    for tfidfSW in [None, 'english']:\n",
    "        for posSW in [True, False]:\n",
    "            for svd in [True, False]:\n",
    "                #print()\n",
    "                X_train_combined, X_test_combined = create_combined(lemm, tfidfSW, (1,2), posSW, svd,  X_train, X_test)\n",
    "                # Instantiate the model\n",
    "                # Instantiate the model\n",
    "                rf = RandomForestClassifier()\n",
    "\n",
    "                # Train the model on the custom training set\n",
    "                rf.fit(X_train_combined, y_train)\n",
    "                # predict the custom test set\n",
    "                y_pred_rf = rf.predict(X_test_combined)\n",
    "                \n",
    "                # scores\n",
    "                f1score = metrics.f1_score(y_test, y_pred_rf, average=None)\n",
    "                acc = metrics.accuracy_score(y_test, y_pred_rf)\n",
    "                \n",
    "                d = {\"PoS_SW\": posSW, \"TFIDF Lemmatization\":lemm,\n",
    "                    \"TFIDF: ngram_range\": (1,2), \"TFIDF_SW\": tfidfSW, \"SVD\": svd,\n",
    "                    \"F1 score\": f1score, \"Accuracy\":acc}\n",
    "                rf_df.append(d)\n",
    "rf_scores = pd.DataFrame(rf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PoS_SW</th>\n",
       "      <th>TFIDF Lemmatization</th>\n",
       "      <th>TFIDF: ngram_range</th>\n",
       "      <th>TFIDF_SW</th>\n",
       "      <th>SVD</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6539618856569709, 0.3002028397565923]</td>\n",
       "      <td>0.536913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7180500658761528, 0.4030683403068341]</td>\n",
       "      <td>0.617002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6679764243614932, 0.2838983050847458]</td>\n",
       "      <td>0.546309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7219834710743801, 0.4179930795847751]</td>\n",
       "      <td>0.623714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.5312899106002553, 0.48090523338048086]</td>\n",
       "      <td>0.507383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7414372061786434, 0.4839142091152816]</td>\n",
       "      <td>0.655481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.5365649811951525, 0.4660568127106403]</td>\n",
       "      <td>0.503803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7399662731871839, 0.4877076411960133]</td>\n",
       "      <td>0.655034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6565087777409738, 0.2853204686423157]</td>\n",
       "      <td>0.536018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7164965426407639, 0.3991625959525471]</td>\n",
       "      <td>0.614765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6596927100359594, 0.26222537207654145]</td>\n",
       "      <td>0.534228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7194197164523574, 0.40779401530967296]</td>\n",
       "      <td>0.619239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.5537154989384289, 0.5030732860520094]</td>\n",
       "      <td>0.529754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7450980392156862, 0.5013227513227513]</td>\n",
       "      <td>0.662640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.6173015307938768, 0.35279951836243234]</td>\n",
       "      <td>0.519016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.7205574912891985, 0.49875]</td>\n",
       "      <td>0.641163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PoS_SW  TFIDF Lemmatization TFIDF: ngram_range TFIDF_SW    SVD  \\\n",
       "0     True                 True             (1, 2)     None   True   \n",
       "1     True                 True             (1, 2)     None  False   \n",
       "2    False                 True             (1, 2)     None   True   \n",
       "3    False                 True             (1, 2)     None  False   \n",
       "4     True                 True             (1, 2)  english   True   \n",
       "5     True                 True             (1, 2)  english  False   \n",
       "6    False                 True             (1, 2)  english   True   \n",
       "7    False                 True             (1, 2)  english  False   \n",
       "8     True                False             (1, 2)     None   True   \n",
       "9     True                False             (1, 2)     None  False   \n",
       "10   False                False             (1, 2)     None   True   \n",
       "11   False                False             (1, 2)     None  False   \n",
       "12    True                False             (1, 2)  english   True   \n",
       "13    True                False             (1, 2)  english  False   \n",
       "14   False                False             (1, 2)  english   True   \n",
       "15   False                False             (1, 2)  english  False   \n",
       "\n",
       "                                     F1 score  Accuracy  \n",
       "0    [0.6539618856569709, 0.3002028397565923]  0.536913  \n",
       "1    [0.7180500658761528, 0.4030683403068341]  0.617002  \n",
       "2    [0.6679764243614932, 0.2838983050847458]  0.546309  \n",
       "3    [0.7219834710743801, 0.4179930795847751]  0.623714  \n",
       "4   [0.5312899106002553, 0.48090523338048086]  0.507383  \n",
       "5    [0.7414372061786434, 0.4839142091152816]  0.655481  \n",
       "6    [0.5365649811951525, 0.4660568127106403]  0.503803  \n",
       "7    [0.7399662731871839, 0.4877076411960133]  0.655034  \n",
       "8    [0.6565087777409738, 0.2853204686423157]  0.536018  \n",
       "9    [0.7164965426407639, 0.3991625959525471]  0.614765  \n",
       "10  [0.6596927100359594, 0.26222537207654145]  0.534228  \n",
       "11  [0.7194197164523574, 0.40779401530967296]  0.619239  \n",
       "12   [0.5537154989384289, 0.5030732860520094]  0.529754  \n",
       "13   [0.7450980392156862, 0.5013227513227513]  0.662640  \n",
       "14  [0.6173015307938768, 0.35279951836243234]  0.519016  \n",
       "15              [0.7205574912891985, 0.49875]  0.641163  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highest Accuracy score for rf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoS_SW                                                     True\n",
       "TFIDF Lemmatization                                       False\n",
       "TFIDF: ngram_range                                       (1, 2)\n",
       "TFIDF_SW                                                english\n",
       "SVD                                                       False\n",
       "F1 score               [0.7450980392156862, 0.5013227513227513]\n",
       "Accuracy                                                0.66264\n",
       "Name: 13, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_scores.loc[rf_scores['Accuracy'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a **3%** improvement in accuracy from the previous rf scores:\n",
    "\n",
    "_F-Measure RF for custom feature vector: 0.73501474 0.42907551\n",
    "Accuracy RF for custom feature vector: 0.6380313199105145_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn VotingClassifier to combine two different clasiffiers and predict the \"most voted\" output from two of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined, X_test_combined = create_combined(lemm=True, tfidfSW=None, ngram_r=(1,2), \n",
    "                                                    posSW=True, svd=False, X_train=X_train, X_test=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_combined, y_train)\n",
    "\n",
    "svm_clf = svm.SVC(C=1, kernel='linear', probability=True)\n",
    "svm_clf.fit(X_train_combined, y_train)\n",
    "\n",
    "est_ensemble = VotingClassifier(estimators=[('RF', rf), ('SVM', svm_clf)],\n",
    "                        voting='soft',\n",
    "                        weights=[1, 1])\n",
    "\n",
    "est_ensemble.fit(X_train_combined, y_train)\n",
    "y_pred_ensemble = est_ensemble.predict(X_test_combined)\n",
    "\n",
    "f1score = metrics.f1_score(y_test, y_pred_ensemble, average=None)\n",
    "acc = metrics.accuracy_score(y_test, y_pred_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure of Voting Classifier for custom feature vector: [0.74586697 0.64633494]\n",
      "Accuracy of Voting Classifier for custom feature vector: 0.7042505592841163\n"
     ]
    }
   ],
   "source": [
    "print(\"F-Measure of Voting Classifier for custom feature vector:\", f1score)\n",
    "print(\"Accuracy of Voting Classifier for custom feature vector:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "voting='soft' predicts the labels as the ones with the maximum probabilities after suming all models predictions.<br>\n",
    "voting='hard' predicts the labels following majority vote rule, i.e. the mode of the models predictions.<br>\n",
    "_( https://stackoverflow.com/questions/22433646/combine-two-different-classifier-result-in-scikit-learn-python )_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another idea..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the different types of preprocessing of the text data did not result in great improvement of scores for the models, another idea is suggested:<br>\n",
    "<br>\n",
    "Creation of a model using only the sparse TFIDF data and then combine its predictions (probabilities) as a dense feature with the PoS dense features to create a model (ie: ensembling via stacking).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No time to try this!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
